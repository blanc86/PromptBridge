1) api_utilities.py:
import os
import requests

class APIUtilities:
    def __init__(self):
        self.gnews_key = os.getenv("GNEWS_KEY")
        self.weather_key = os.getenv("OPENWEATHERMAP_KEY")
        self.timezonedb_key = os.getenv("TIMEZONEDB_API_KEY")
        self.fyers_token = os.getenv("FYERS_API_KEY")
        self.kite_token = os.getenv("ZERODHA_KITE_API_KEY")

    def get_news(self, query="India", lang="en", max_results=5):
        if not self.gnews_key:
            return ["❌ GNews API key not found."]
        
        url = f"https://gnews.io/api/v4/search?q={query}&lang={lang}&max={max_results}&apikey={self.gnews_key}"
        try:
            response = requests.get(url, timeout=10)
            data = response.json()
            return [f"{a['title']} ({a['source']['name']})" for a in data.get("articles", [])] or ["No news found."]
        except requests.exceptions.RequestException as e:
            return [f"🌐 GNews API Error: {e}"]

    def get_weather(self, city="Delhi"):
        if not self.weather_key:
            return {"error": "❌ OpenWeatherMap API key not found."}
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={self.weather_key}&units=metric"
        try:
            return requests.get(url, timeout=10).json()
        except requests.exceptions.RequestException as e:
            return {"error": f"🌦️ Weather API Error: {e}"}

    def get_time_by_timezone(self, timezone="Asia/Kolkata"):
        if not self.timezonedb_key:
            return {"error": "❌ TimeZoneDB API key not found."}
        url = f"http://api.timezonedb.com/v2.1/get-time-zone?key={self.timezonedb_key}&format=json&by=zone&zone={timezone}"
        try:
            return requests.get(url, timeout=10).json()
        except requests.exceptions.RequestException as e:
            return {"error": f"🕒 Time API Error: {e}"}

    def get_quote(self):
        try:
            data = requests.get("https://zenquotes.io/api/random", timeout=5).json()
            return data[0]["q"] + " - " + data[0]["a"]
        except Exception as e:
            return f"📜 Quote fetch failed: {e}"

    def get_fun_fact(self):
        try:
            return requests.get("https://uselessfacts.jsph.pl/random.json?language=en", timeout=5).json().get("text", "No fact found.")
        except Exception as e:
            return f"🧠 Fun fact fetch failed: {e}"

    def get_word_definition(self, word):
        try:
            return requests.get(f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}", timeout=5).json()
        except Exception as e:
            return {"error": f"📘 Dictionary error: {e}"}

    def get_fyers_data(self):
        if not self.fyers_token:
            return {"error": "❌ Fyers token not found."}
        headers = {"Authorization": f"Bearer {self.fyers_token}"}
        try:
            return requests.get("https://api.fyers.in/api/v2/market-status", headers=headers, timeout=5).json()
        except Exception as e:
            return {"error": f"📈 Fyers error: {e}"}

    def get_zerodha_data(self):
        if not self.kite_token:
            return {"error": "❌ Zerodha token not found."}
        headers = {"X-Kite-Version": "3", "Authorization": f"token {self.kite_token}"}
        try:
            return requests.get("https://api.kite.trade/margins/equity", headers=headers, timeout=5).json()
        except Exception as e:
            return {"error": f"📊 Zerodha error: {e}"}

# === For use in main.py ===
api_client = APIUtilities()

def fetch_news(query="India"):
    return api_client.get_news(query)

def fetch_weather(city="Delhi"):
    return api_client.get_weather(city)

def fetch_quote():
    return api_client.get_quote()

def fetch_fun_fact():
    return api_client.get_fun_fact()

def fetch_definition(word="technology"):
    return api_client.get_word_definition(word)

def fetch_time(timezone="Asia/Kolkata"):
    return api_client.get_time_by_timezone(timezone)
# Final Copy

2) gemini_chat.py:
import os
import google.generativeai as genai

class GeminiChat:
    def __init__(self, model_name="models/gemini-1.5-flash-latest"):
        # Initializes Gemini API and starts a chat session
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("❌ GEMINI_API_KEY not found in environment variables.")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.chat = self.model.start_chat(history=[])
        print("✅ Gemini model initialized.")

    def send(self, message: str) -> str:
        # Sends a message to the Gemini model and returns the response
        response = self.chat.send_message(message)
        return response.text.strip()

    def reset_chat(self):
        # Resets the chat history
        self.chat = self.model.start_chat(history=[])

# === Function for use in main.py ===
gemini_instance = None

def get_gemini_response(prompt: str) -> str:
    # Returns the response from the Gemini model
    global gemini_instance
    if gemini_instance is None:
        gemini_instance = GeminiChat()
    return gemini_instance.send(prompt)

# === Optional CLI test ===
if __name__ == "__main__":
    gemini = GeminiChat()
    print("💬 Type 'exit' to stop chatting.")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        reply = gemini.send(user_input)
        print("\nBot:", reply, "\n")
#Final copy

3) prompt_optimizer.py
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Optional: Load BART model from Hugging Face
try:
    from transformers import pipeline
    summarizer = pipeline("summarization", model="facebook/bart-base")
except ImportError:
    summarizer = None

# === SIMPLE SENTENCE SPLITTER ===

def split_sentences(text: str) -> list:
    return re.split(r'(?<=[.!?]) +', text.strip())

# === STOPWORD REMOVER (Optional Enhancement) ===

def remove_stopwords(text: str) -> str:
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
    words = text.split()
    return ' '.join([word for word in words if word.lower() not in ENGLISH_STOP_WORDS])

# === KEYWORD HIGHLIGHTING (Optional Debug Tool) ===

def extract_keywords(text: str, top_k: int = 5) -> list:
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform([text])
    indices = np.argsort(X.toarray()[0])[::-1]
    features = np.array(vectorizer.get_feature_names_out())
    return features[indices][:top_k].tolist()

# === SUMMARIZER (Aggressive Extractive or BART if available) ===

def summarize_text(text: str, num_sentences: int = 3) -> str:
    """
    Summarizes the text using BART if available; otherwise falls back to TF-IDF extractive method.
    """
    if summarizer:
        try:
            result = summarizer(text, max_length=120, min_length=60, do_sample=False)
            return result[0]['summary_text']
        except Exception:
            pass  # fallback in case BART fails

    # === FALLBACK: TF-IDF summarizer ===
    sentences = split_sentences(text)
    if len(sentences) <= num_sentences:
        return text

    tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.9)
    sentence_vectors = tfidf.fit_transform(sentences)
    sentence_scores = sentence_vectors.sum(axis=1).A1
    ranked_indices = np.argsort(sentence_scores)[::-1][:num_sentences]
    ranked_indices.sort()
    summary = ' '.join([sentences[i] for i in ranked_indices])
    return summary

# === CLEANER (Friendly Cleanup) ===

def friendly_clean(prompt: str) -> str:
    prompt = re.sub(r"\s{2,}", " ", prompt.strip())  # Remove extra spaces
    prompt = re.sub(r"\n+", " ", prompt)             # Replace newlines with spaces
    return prompt

# === TOOL INPUT OPTIMIZER ===

def optimize_tool_input(prompt: str) -> str:
    cleaned = friendly_clean(prompt)
    if len(cleaned.split()) > 75:
        return summarize_text(cleaned, num_sentences=2)
    return cleaned

# === FOR IMPORT IN MAIN ===

def get_optimized_prompt_and_keywords(prompt: str):
    optimized = optimize_tool_input(prompt)
    keywords = extract_keywords(optimized)
    return optimized, keywords

# === EXAMPLE TEST ===

if __name__ == "__main__":
    sample_prompt = (
        """
        Artificial intelligence is a field of computer science that focuses on creating intelligent machines that work and react like humans. 
        Some of the activities computers with artificial intelligence are designed for include: speech recognition, learning, planning, and problem-solving. 
        As technology advances, artificial intelligence has become more integrated into daily life, from virtual assistants to recommendation algorithms. 
        AI also plays a crucial role in data analysis, helping businesses make data-driven decisions. 
        Despite its many benefits, AI raises ethical concerns such as job displacement, privacy, and biases in decision-making systems.
        """
    )

    print("\n--- Optimized Tool Input ---")
    optimized, keywords = get_optimized_prompt_and_keywords(sample_prompt)
    print(optimized)

    print("\n--- Extracted Keywords ---")
    print(keywords)
# Final Copy

4) speech_to_text:
import sounddevice as sd
import numpy as np
import threading
from faster_whisper import WhisperModel
from typing import Optional
import torch

# Audio parameters
sample_rate = 16000
block_duration = 5  # seconds

# Global model (lazy-loaded)
model = None

# Globals
final_transcript = []
stop_recording = threading.Event()

def load_whisper():
    global model
    if model is None:
        print("🎤 Loading Whisper model...")
        model = WhisperModel("medium", compute_type="float16", device="cuda")
        print("✅ Whisper model loaded.")

def unload_whisper():
    global model
    if model:
        print("🧹 Unloading Whisper model...")
        del model
        model = None
        torch.cuda.empty_cache()
        print("✅ Whisper model unloaded.")

def transcribe_block():
    global model
    audio = sd.rec(int(sample_rate * block_duration), samplerate=sample_rate, channels=1)
    sd.wait()
    audio_array = np.squeeze(audio)

    if not np.any(audio_array):
        print("⚠️ Skipping silent audio block")
        return

    print("📡 Transcribing...")
    segments_gen, _ = model.transcribe(
        audio_array,
        language=None,
        beam_size=10,
        vad_filter=True,
        vad_parameters={"threshold": 0.5}
    )

    for segment in segments_gen:
        text = segment.text.strip()
        if text:
            print(f"🗣️ {text}")
            final_transcript.append(text)

def record_loop():
    print("\n🎤 Recording started. Speak now!")
    print("🛑 Press [e] then Enter at any time to stop.\n")
    while not stop_recording.is_set():
        transcribe_block()

def run_button_based_transcription() -> Optional[str]:
    global final_transcript
    final_transcript = []

    while True:
        choice = input("\n🎮 Press [s] to start, [e] to exit: ").lower().strip()
        if choice == "s":
            load_whisper()
            stop_recording.clear()
            recording_thread = threading.Thread(target=record_loop)
            recording_thread.start()

            while True:
                end_cmd = input().strip().lower()
                if end_cmd == "e":
                    stop_recording.set()
                    recording_thread.join()
                    break

            full_transcript = " ".join(final_transcript)
            print("\n📝 Final Transcript:")
            print(full_transcript)
            unload_whisper()
            return full_transcript

        elif choice == "e":
            print("👋 Exiting without recording.")
            unload_whisper()
            return None

        else:
            print("❌ Invalid input. Use [s] to start, [e] to exit.")

# If run directly
if __name__ == "__main__":
    run_button_based_transcription()



5) text_to_speech.py
from gtts import gTTS
from pydub import AudioSegment
from pydub.playback import play
from langdetect import detect
import io

def speak(text: str, lang: str = None):
    # Converts text to speech, detecting language if not specified
    if lang is None:
        lang = detect(text)
    try:
        tts = gTTS(text=text, lang=lang)
        with io.BytesIO() as f:
            tts.write_to_fp(f)
            f.seek(0)
            audio = AudioSegment.from_file(f, format="mp3")
            play(audio)
    except ValueError:
        print(f"❌ Language '{lang}' not supported by gTTS.")

# Only runs when the file is executed directly
if __name__ == "__main__":
    # Example: Speak a phrase in any detectable language
    speak("तुम्हारा नाम क्या है?")
    speak("What is your name?")
    speak("আপনার নাম কি?")

# Final Copy

6) translator.py
from transformers import NllbTokenizer, AutoModelForSeq2SeqLM
import torch
from langdetect import detect

class NLLBTranslator:
    def __init__(self, model_name="facebook/nllb-200-distilled-600M"):
        # Initializes the model and tokenizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"🧠 Initializing model on {self.device}...")

        self.tokenizer = NllbTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)
        print("✅ NLLB Model and Tokenizer loaded.")

        self.lang_code_to_id = self.tokenizer.convert_tokens_to_ids

        # Adjusted language detection map based on supported language codes
        self.lang_detect_map = {
            "as": "asm_Beng",
            "bn": "ben_Beng",
            "brx": "npi_Deva",
            "doi": "hin_Deva",
            "en": "eng_Latn",
            "gom": "mar_Deva",
            "gu": "guj_Gujr",
            "hi": "hin_Deva",
            "kn": "kan_Knda",
            "ks": "urd_Arab",
            "mai": "hin_Deva",
            "ml": "mal_Mlym",
            "mr": "mar_Deva",
            "ne": "npi_Deva",
            "pa": "pan_Guru",
            "sa": "san_Deva",
            "sd": "snd_Arab",
            "ta": "tam_Taml",
            "te": "tel_Telu",
            "ur": "urd_Arab"
        }

    def detect_lang_code(self, text):
        # Detects the language code of the input text
        lang = detect(text)
        return self.lang_detect_map.get(lang, "eng_Latn")

    def translate_to_english(self, text):
        # Translates the input text to English
        source_lang = self.detect_lang_code(text)
        print(f"🌐 Translating from {source_lang} → eng_Latn...")
        print(f"🔤 Input text: {text}")
        return self._translate(text, source_lang, "eng_Latn")

    def _translate(self, text, source_lang, target_lang):
        # Internal function to translate text from source to target language
        self.tokenizer.src_lang = source_lang
        encoded = self.tokenizer(text, return_tensors="pt").to(self.device)

        target_lang_id = self.lang_code_to_id(target_lang)
        if target_lang_id is None:
            raise ValueError(f"❌ Invalid target language code: {target_lang}")

        generated_tokens = self.model.generate(
            **encoded,
            forced_bos_token_id=target_lang_id,
            max_length=256
        )

        translated = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        print(f"📝 Translated text: {translated}")
        return translated

# === Function for use in main.py ===
translator_instance = None

def get_translated_text(text):
    # Returns the English translation of the given text
    global translator_instance
    if translator_instance is None:
        translator_instance = NLLBTranslator()
    return translator_instance.translate_to_english(text)


if __name__ == "__main__":
    translator = NLLBTranslator()

    test_cases = {
        "as": "আপুনাৰ নাম কি?",
        "bn": "তোমার নাম কী?",
        "brx": "नङाइ जोनाय हो?",
        "doi": "तूहाडा नाँव की ऐ?",
        "en": "What is your name?",
        "gom": "तुझे नाव किते?",
        "gu": "તમારું નામ શું છે?",
        "hi": "तुम्हारा नाम क्या है?",
        "kn": "ನಿಮ್ಮ ಹೆಸರೇನು?",
        "ks": "تُهند ناو کٔیہ چھُ?",
        "mai": "अहाँक नाम की अछि?",
        "ml": "നിന്റെ പേര് എന്താണ്?",
        "mr": "तुझे नाव काय आहे?",
        "ne": "तिम्रो नाम के हो?",
        "pa": "ਤੁਹਾਡਾ ਨਾਂ ਕੀ ਹੈ?",
        "sa": "तव नाम किम् अस्ति?",
        "sd": "توھانجو نالو ڇا آهي؟",
        "ta": "உங்கள் பெயர் என்ன?",
        "te": "మీ పేరు ఏమిటి?",
        "ur": "آپ کا نام کیا ہے؟"
    }

    for lang, input_text in test_cases.items():
        print(f"\n🔎 Testing for language code: {lang}")
        try:
            translator.translate_to_english(input_text)
        except Exception as e:
            print(f"❌ Error: {e}")
# Final Copy