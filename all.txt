1) api_utilities.py:
import os
import requests

class APIUtilities:
    def __init__(self):
        self.gnews_key = os.getenv("GNEWS_KEY")
        self.weather_key = os.getenv("OPENWEATHERMAP_KEY")
        self.timezonedb_key = os.getenv("TIMEZONEDB_API_KEY")
        self.fyers_token = os.getenv("FYERS_API_KEY")
        self.kite_token = os.getenv("ZERODHA_KITE_API_KEY")

    def get_news(self, query="India", lang="en", max_results=5):
        if not self.gnews_key:
            return ["âŒ GNews API key not found."]
        
        url = f"https://gnews.io/api/v4/search?q={query}&lang={lang}&max={max_results}&apikey={self.gnews_key}"
        try:
            response = requests.get(url, timeout=10)
            data = response.json()
            return [f"{a['title']} ({a['source']['name']})" for a in data.get("articles", [])] or ["No news found."]
        except requests.exceptions.RequestException as e:
            return [f"ğŸŒ GNews API Error: {e}"]

    def get_weather(self, city="Delhi"):
        if not self.weather_key:
            return {"error": "âŒ OpenWeatherMap API key not found."}
        url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={self.weather_key}&units=metric"
        try:
            return requests.get(url, timeout=10).json()
        except requests.exceptions.RequestException as e:
            return {"error": f"ğŸŒ¦ï¸ Weather API Error: {e}"}

    def get_time_by_timezone(self, timezone="Asia/Kolkata"):
        if not self.timezonedb_key:
            return {"error": "âŒ TimeZoneDB API key not found."}
        url = f"http://api.timezonedb.com/v2.1/get-time-zone?key={self.timezonedb_key}&format=json&by=zone&zone={timezone}"
        try:
            return requests.get(url, timeout=10).json()
        except requests.exceptions.RequestException as e:
            return {"error": f"ğŸ•’ Time API Error: {e}"}

    def get_quote(self):
        try:
            data = requests.get("https://zenquotes.io/api/random", timeout=5).json()
            return data[0]["q"] + " - " + data[0]["a"]
        except Exception as e:
            return f"ğŸ“œ Quote fetch failed: {e}"

    def get_fun_fact(self):
        try:
            return requests.get("https://uselessfacts.jsph.pl/random.json?language=en", timeout=5).json().get("text", "No fact found.")
        except Exception as e:
            return f"ğŸ§  Fun fact fetch failed: {e}"

    def get_word_definition(self, word):
        try:
            return requests.get(f"https://api.dictionaryapi.dev/api/v2/entries/en/{word}", timeout=5).json()
        except Exception as e:
            return {"error": f"ğŸ“˜ Dictionary error: {e}"}

    def get_fyers_data(self):
        if not self.fyers_token:
            return {"error": "âŒ Fyers token not found."}
        headers = {"Authorization": f"Bearer {self.fyers_token}"}
        try:
            return requests.get("https://api.fyers.in/api/v2/market-status", headers=headers, timeout=5).json()
        except Exception as e:
            return {"error": f"ğŸ“ˆ Fyers error: {e}"}

    def get_zerodha_data(self):
        if not self.kite_token:
            return {"error": "âŒ Zerodha token not found."}
        headers = {"X-Kite-Version": "3", "Authorization": f"token {self.kite_token}"}
        try:
            return requests.get("https://api.kite.trade/margins/equity", headers=headers, timeout=5).json()
        except Exception as e:
            return {"error": f"ğŸ“Š Zerodha error: {e}"}

# === For use in main.py ===
api_client = APIUtilities()

def fetch_news(query="India"):
    return api_client.get_news(query)

def fetch_weather(city="Delhi"):
    return api_client.get_weather(city)

def fetch_quote():
    return api_client.get_quote()

def fetch_fun_fact():
    return api_client.get_fun_fact()

def fetch_definition(word="technology"):
    return api_client.get_word_definition(word)

def fetch_time(timezone="Asia/Kolkata"):
    return api_client.get_time_by_timezone(timezone)
# Final Copy

2) gemini_chat.py:
import os
import google.generativeai as genai

class GeminiChat:
    def __init__(self, model_name="models/gemini-1.5-flash-latest"):
        # Initializes Gemini API and starts a chat session
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("âŒ GEMINI_API_KEY not found in environment variables.")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.chat = self.model.start_chat(history=[])
        print("âœ… Gemini model initialized.")

    def send(self, message: str) -> str:
        # Sends a message to the Gemini model and returns the response
        response = self.chat.send_message(message)
        return response.text.strip()

    def reset_chat(self):
        # Resets the chat history
        self.chat = self.model.start_chat(history=[])

# === Function for use in main.py ===
gemini_instance = None

def get_gemini_response(prompt: str) -> str:
    # Returns the response from the Gemini model
    global gemini_instance
    if gemini_instance is None:
        gemini_instance = GeminiChat()
    return gemini_instance.send(prompt)

# === Optional CLI test ===
if __name__ == "__main__":
    gemini = GeminiChat()
    print("ğŸ’¬ Type 'exit' to stop chatting.")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        reply = gemini.send(user_input)
        print("\nBot:", reply, "\n")
#Final copy

3) prompt_optimizer.py
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Optional: Load BART model from Hugging Face
try:
    from transformers import pipeline
    summarizer = pipeline("summarization", model="facebook/bart-base")
except ImportError:
    summarizer = None

# === SIMPLE SENTENCE SPLITTER ===

def split_sentences(text: str) -> list:
    return re.split(r'(?<=[.!?]) +', text.strip())

# === STOPWORD REMOVER (Optional Enhancement) ===

def remove_stopwords(text: str) -> str:
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
    words = text.split()
    return ' '.join([word for word in words if word.lower() not in ENGLISH_STOP_WORDS])

# === KEYWORD HIGHLIGHTING (Optional Debug Tool) ===

def extract_keywords(text: str, top_k: int = 5) -> list:
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform([text])
    indices = np.argsort(X.toarray()[0])[::-1]
    features = np.array(vectorizer.get_feature_names_out())
    return features[indices][:top_k].tolist()

# === SUMMARIZER (Aggressive Extractive or BART if available) ===

def summarize_text(text: str, num_sentences: int = 3) -> str:
    """
    Summarizes the text using BART if available; otherwise falls back to TF-IDF extractive method.
    """
    if summarizer:
        try:
            result = summarizer(text, max_length=120, min_length=60, do_sample=False)
            return result[0]['summary_text']
        except Exception:
            pass  # fallback in case BART fails

    # === FALLBACK: TF-IDF summarizer ===
    sentences = split_sentences(text)
    if len(sentences) <= num_sentences:
        return text

    tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.9)
    sentence_vectors = tfidf.fit_transform(sentences)
    sentence_scores = sentence_vectors.sum(axis=1).A1
    ranked_indices = np.argsort(sentence_scores)[::-1][:num_sentences]
    ranked_indices.sort()
    summary = ' '.join([sentences[i] for i in ranked_indices])
    return summary

# === CLEANER (Friendly Cleanup) ===

def friendly_clean(prompt: str) -> str:
    prompt = re.sub(r"\s{2,}", " ", prompt.strip())  # Remove extra spaces
    prompt = re.sub(r"\n+", " ", prompt)             # Replace newlines with spaces
    return prompt

# === TOOL INPUT OPTIMIZER ===

def optimize_tool_input(prompt: str) -> str:
    cleaned = friendly_clean(prompt)
    if len(cleaned.split()) > 75:
        return summarize_text(cleaned, num_sentences=2)
    return cleaned

# === FOR IMPORT IN MAIN ===

def get_optimized_prompt_and_keywords(prompt: str):
    optimized = optimize_tool_input(prompt)
    keywords = extract_keywords(optimized)
    return optimized, keywords

# === EXAMPLE TEST ===

if __name__ == "__main__":
    sample_prompt = (
        """
        Artificial intelligence is a field of computer science that focuses on creating intelligent machines that work and react like humans. 
        Some of the activities computers with artificial intelligence are designed for include: speech recognition, learning, planning, and problem-solving. 
        As technology advances, artificial intelligence has become more integrated into daily life, from virtual assistants to recommendation algorithms. 
        AI also plays a crucial role in data analysis, helping businesses make data-driven decisions. 
        Despite its many benefits, AI raises ethical concerns such as job displacement, privacy, and biases in decision-making systems.
        """
    )

    print("\n--- Optimized Tool Input ---")
    optimized, keywords = get_optimized_prompt_and_keywords(sample_prompt)
    print(optimized)

    print("\n--- Extracted Keywords ---")
    print(keywords)
# Final Copy

4) speech_to_text:
import sounddevice as sd
import numpy as np
import threading
from faster_whisper import WhisperModel
from typing import Optional
import torch

# Audio parameters
sample_rate = 16000
block_duration = 5  # seconds

# Global model (lazy-loaded)
model = None

# Globals
final_transcript = []
stop_recording = threading.Event()

def load_whisper():
    global model
    if model is None:
        print("ğŸ¤ Loading Whisper model...")
        model = WhisperModel("medium", compute_type="float16", device="cuda")
        print("âœ… Whisper model loaded.")

def unload_whisper():
    global model
    if model:
        print("ğŸ§¹ Unloading Whisper model...")
        del model
        model = None
        torch.cuda.empty_cache()
        print("âœ… Whisper model unloaded.")

def transcribe_block():
    global model
    audio = sd.rec(int(sample_rate * block_duration), samplerate=sample_rate, channels=1)
    sd.wait()
    audio_array = np.squeeze(audio)

    if not np.any(audio_array):
        print("âš ï¸ Skipping silent audio block")
        return

    print("ğŸ“¡ Transcribing...")
    segments_gen, _ = model.transcribe(
        audio_array,
        language=None,
        beam_size=10,
        vad_filter=True,
        vad_parameters={"threshold": 0.5}
    )

    for segment in segments_gen:
        text = segment.text.strip()
        if text:
            print(f"ğŸ—£ï¸ {text}")
            final_transcript.append(text)

def record_loop():
    print("\nğŸ¤ Recording started. Speak now!")
    print("ğŸ›‘ Press [e] then Enter at any time to stop.\n")
    while not stop_recording.is_set():
        transcribe_block()

def run_button_based_transcription() -> Optional[str]:
    global final_transcript
    final_transcript = []

    while True:
        choice = input("\nğŸ® Press [s] to start, [e] to exit: ").lower().strip()
        if choice == "s":
            load_whisper()
            stop_recording.clear()
            recording_thread = threading.Thread(target=record_loop)
            recording_thread.start()

            while True:
                end_cmd = input().strip().lower()
                if end_cmd == "e":
                    stop_recording.set()
                    recording_thread.join()
                    break

            full_transcript = " ".join(final_transcript)
            print("\nğŸ“ Final Transcript:")
            print(full_transcript)
            unload_whisper()
            return full_transcript

        elif choice == "e":
            print("ğŸ‘‹ Exiting without recording.")
            unload_whisper()
            return None

        else:
            print("âŒ Invalid input. Use [s] to start, [e] to exit.")

# If run directly
if __name__ == "__main__":
    run_button_based_transcription()



5) text_to_speech.py
from gtts import gTTS
from pydub import AudioSegment
from pydub.playback import play
from langdetect import detect
import io

def speak(text: str, lang: str = None):
    # Converts text to speech, detecting language if not specified
    if lang is None:
        lang = detect(text)
    try:
        tts = gTTS(text=text, lang=lang)
        with io.BytesIO() as f:
            tts.write_to_fp(f)
            f.seek(0)
            audio = AudioSegment.from_file(f, format="mp3")
            play(audio)
    except ValueError:
        print(f"âŒ Language '{lang}' not supported by gTTS.")

# Only runs when the file is executed directly
if __name__ == "__main__":
    # Example: Speak a phrase in any detectable language
    speak("à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¤¾ à¤¨à¤¾à¤® à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?")
    speak("What is your name?")
    speak("à¦†à¦ªà¦¨à¦¾à¦° à¦¨à¦¾à¦® à¦•à¦¿?")

# Final Copy

6) translator.py
from transformers import NllbTokenizer, AutoModelForSeq2SeqLM
import torch
from langdetect import detect

class NLLBTranslator:
    def __init__(self, model_name="facebook/nllb-200-distilled-600M"):
        # Initializes the model and tokenizer
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ§  Initializing model on {self.device}...")

        self.tokenizer = NllbTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)
        print("âœ… NLLB Model and Tokenizer loaded.")

        self.lang_code_to_id = self.tokenizer.convert_tokens_to_ids

        # Adjusted language detection map based on supported language codes
        self.lang_detect_map = {
            "as": "asm_Beng",
            "bn": "ben_Beng",
            "brx": "npi_Deva",
            "doi": "hin_Deva",
            "en": "eng_Latn",
            "gom": "mar_Deva",
            "gu": "guj_Gujr",
            "hi": "hin_Deva",
            "kn": "kan_Knda",
            "ks": "urd_Arab",
            "mai": "hin_Deva",
            "ml": "mal_Mlym",
            "mr": "mar_Deva",
            "ne": "npi_Deva",
            "pa": "pan_Guru",
            "sa": "san_Deva",
            "sd": "snd_Arab",
            "ta": "tam_Taml",
            "te": "tel_Telu",
            "ur": "urd_Arab"
        }

    def detect_lang_code(self, text):
        # Detects the language code of the input text
        lang = detect(text)
        return self.lang_detect_map.get(lang, "eng_Latn")

    def translate_to_english(self, text):
        # Translates the input text to English
        source_lang = self.detect_lang_code(text)
        print(f"ğŸŒ Translating from {source_lang} â†’ eng_Latn...")
        print(f"ğŸ”¤ Input text: {text}")
        return self._translate(text, source_lang, "eng_Latn")

    def _translate(self, text, source_lang, target_lang):
        # Internal function to translate text from source to target language
        self.tokenizer.src_lang = source_lang
        encoded = self.tokenizer(text, return_tensors="pt").to(self.device)

        target_lang_id = self.lang_code_to_id(target_lang)
        if target_lang_id is None:
            raise ValueError(f"âŒ Invalid target language code: {target_lang}")

        generated_tokens = self.model.generate(
            **encoded,
            forced_bos_token_id=target_lang_id,
            max_length=256
        )

        translated = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        print(f"ğŸ“ Translated text: {translated}")
        return translated

# === Function for use in main.py ===
translator_instance = None

def get_translated_text(text):
    # Returns the English translation of the given text
    global translator_instance
    if translator_instance is None:
        translator_instance = NLLBTranslator()
    return translator_instance.translate_to_english(text)


if __name__ == "__main__":
    translator = NLLBTranslator()

    test_cases = {
        "as": "à¦†à¦ªà§à¦¨à¦¾à§° à¦¨à¦¾à¦® à¦•à¦¿?",
        "bn": "à¦¤à§‹à¦®à¦¾à¦° à¦¨à¦¾à¦® à¦•à§€?",
        "brx": "à¤¨à¤™à¤¾à¤‡ à¤œà¥‹à¤¨à¤¾à¤¯ à¤¹à¥‹?",
        "doi": "à¤¤à¥‚à¤¹à¤¾à¤¡à¤¾ à¤¨à¤¾à¤à¤µ à¤•à¥€ à¤?",
        "en": "What is your name?",
        "gom": "à¤¤à¥à¤à¥‡ à¤¨à¤¾à¤µ à¤•à¤¿à¤¤à¥‡?",
        "gu": "àª¤àª®àª¾àª°à«àª‚ àª¨àª¾àª® àª¶à«àª‚ àª›à«‡?",
        "hi": "à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¤¾ à¤¨à¤¾à¤® à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ?",
        "kn": "à²¨à²¿à²®à³à²® à²¹à³†à²¸à²°à³‡à²¨à³?",
        "ks": "ØªÙÙ‡Ù†Ø¯ Ù†Ø§Ùˆ Ú©Ù”ÛŒÛ Ú†Ú¾Ù?",
        "mai": "à¤…à¤¹à¤¾à¤à¤• à¤¨à¤¾à¤® à¤•à¥€ à¤…à¤›à¤¿?",
        "ml": "à´¨à´¿à´¨àµà´±àµ† à´ªàµ‡à´°àµ à´à´¨àµà´¤à´¾à´£àµ?",
        "mr": "à¤¤à¥à¤à¥‡ à¤¨à¤¾à¤µ à¤•à¤¾à¤¯ à¤†à¤¹à¥‡?",
        "ne": "à¤¤à¤¿à¤®à¥à¤°à¥‹ à¤¨à¤¾à¤® à¤•à¥‡ à¤¹à¥‹?",
        "pa": "à¨¤à©à¨¹à¨¾à¨¡à¨¾ à¨¨à¨¾à¨‚ à¨•à©€ à¨¹à©ˆ?",
        "sa": "à¤¤à¤µ à¤¨à¤¾à¤® à¤•à¤¿à¤®à¥ à¤…à¤¸à¥à¤¤à¤¿?",
        "sd": "ØªÙˆÚ¾Ø§Ù†Ø¬Ùˆ Ù†Ø§Ù„Ùˆ Ú‡Ø§ Ø¢Ù‡ÙŠØŸ",
        "ta": "à®‰à®™à¯à®•à®³à¯ à®ªà¯†à®¯à®°à¯ à®à®©à¯à®©?",
        "te": "à°®à±€ à°ªà±‡à°°à± à°à°®à°¿à°Ÿà°¿?",
        "ur": "Ø¢Ù¾ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’ØŸ"
    }

    for lang, input_text in test_cases.items():
        print(f"\nğŸ” Testing for language code: {lang}")
        try:
            translator.translate_to_english(input_text)
        except Exception as e:
            print(f"âŒ Error: {e}")
# Final Copy